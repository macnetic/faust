@inproceedings{Achlioptas2001,
address = {New York, New York, USA},
author = {Achlioptas, Dimitris},
booktitle = {Principles of database systems (PODS)},
doi = {10.1145/375551.375608},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Achlioptas/2001 - Database-friendly random projections.pdf:pdf},
isbn = {1581133618},
pages = {274--281},
publisher = {ACM Press},
title = {{Database-friendly random projections}},
url = {http://portal.acm.org/citation.cfm?doid=375551.375608},
year = {2001}
}
@techreport{Ahrendt2005,
author = {Ahrendt, Peter},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Ahrendt/2005 - The Multivariate Gaussian Probability Distribution.pdf:pdf},
institution = {IMM, Technical University of Denmark},
title = {{The Multivariate Gaussian Probability Distribution}},
year = {2005}
}
@inproceedings{Anaraki2014,
author = {Anaraki, Farhad Pourkamali and Hughes, Shannon M.},
booktitle = {IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Anaraki, Hughes/2014 - Efficient Recovery of Principal Components from Compressive Measurements with Application to Gaussian Mixture Model Estimation.pdf:pdf},
pages = {2351--2355},
title = {{Efficient Recovery of Principal Components from Compressive Measurements with Application to Gaussian Mixture Model Estimation}},
year = {2014}
}
@article{Anderson2013,
annote = {condition on the means for recovering mixture of same variance: blessing of diemnsionality. linear map of a product distribution, then tensor decomposition with ICA, for recovering.
}
@article{LeMagoarou2016,
author={L. Le Magoarou and R. Gribonval},
journal={IEEE Journal of Selected Topics in Signal Processing},
title={Flexible Multilayer Sparse Approximations of Matrices and Applications},
year={2016},
volume={10},
number={4},
pages={688-700},
keywords={approximation theory;computational complexity;concave programming;learning (artificial intelligence);matrix decomposition;signal processing;sparse matrices;computational cost;corresponding matrix;dictionary learning;flexible multilayer sparse approximations;image denoising;inverse problems;linear operators;machine learning techniques;nonconvex optimization;signal processing;Approximation algorithms;Complexity theory;Dictionaries;Inverse problems;Optimization;Sparse matrices;Transforms;Sparse representations;dictionary learning;fast algorithms;image denoising;inverse problems;low complexity},
doi={10.1109/JSTSP.2016.2543461},
ISSN={1932-4553},
month={June},}

























































very large mixture can be polynomially learned (in time and sample), under consition that may fails in low dimension},
author = {Anderson, Joseph and Belkin, Mikhail and Goyal, Navin and Rademacher, Luis and Voss, James},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Anderson et al/2013 - The more, the merrier the blessing of dimensionality for learning large gaussian mixtures.pdf:pdf},
journal = {arXiv:1311.2891},
keywords = {blessing of dimensionality,gaussian mixture models,independent component analysis,sis,smoothed analy-,tensor methods},
pages = {1--30},
title = {{The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures}},
url = {http://arxiv.org/abs/1311.2891},
volume = {35},
year = {2013}
}
@article{Aronzajn1950,
author = {Aronzajn, Nachman},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Aronzajn/1950 - Theory of reproducing kernels.pdf:pdf},
isbn = {047021211X},
journal = {Transactions of the American Mathematical Society},
pages = {337--404},
title = {{Theory of reproducing kernels}},
volume = {68},
year = {1950}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Baraniuk/2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://omni.isr.ist.utl.pt/{~}aguiar/CS{\_}notes.pdf},
year = {2007}
}
@article{Baraniuk2008,
abstract = {We give a simple technique for verifying the Restricted Isometry Property (as introduced by Cand{\`{e}}s and Tao) for random matrices that underlies Compressed Sensing. Our approach has two main ingredients: (i) concentration inequalities for random inner products that have recently provided algorithmically simple proofs of the Johnson–Lindenstrauss lemma; and (ii) covering numbers for finite-dimensional balls in Euclidean space. This leads to an elementary proof of the Restricted Isometry Property and brings out connections between Compressed Sensing and the Johnson–Lindenstrauss lemma. As a result, we obtain simple and direct proofs of Kashin's theorems on widths of finite balls in Euclidean space (and their improvements due to Gluskin) and proofs of the existence of optimal Compressed Sensing measurement matrices. In the process, we also prove that these measurements have a certain universality with respect to the sparsity-inducing basis.},
author = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
doi = {10.1007/s00365-007-9003-x},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Baraniuk et al/2008 - A simple proof of the restricted isometry property for random matrices.pdf:pdf},
isbn = {0176427614320940},
issn = {01764276},
journal = {Constructive Approximation},
keywords = {Compressed sensing,Concentration inequalities,Random matrices,Sampling},
number = {3},
pages = {253--263},
title = {{A simple proof of the restricted isometry property for random matrices}},
volume = {28},
year = {2008}
}
@article{Bashan2008,
abstract = {We consider the problem of estimating and detecting sparse signals over a large area of an image or other medium. We introduce a novel cost function that captures the tradeoff between allocating energy to signal regions, called regions of interest (ROI), versus exploration of other regions. We show that minimizing our cost guarantees reduction of both the error probability over the unknown ROI and the mean square error (MSE) in estimating the ROI content. Two solutions to the resource allocation problem, subject to a total resource constraint, are derived. Asymptotic analysis shows that the estimated ROI converges to the true ROI. We show that our adaptive sampling method outperforms exhaustive search and are nearly optimal in terms of MSE performance. An illustrative example of our method in radar imaging is given.},
author = {Bashan, Eran and Raich, Raviv and Hero, Alfred O.},
doi = {10.1109/TSP.2008.929114},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bashan, Raich, Hero/2008 - Optimal two-stage search for sparse targets using convex criteria.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Adaptive sampling,Adaptive sensing,Energy allocation,Search methods,Sparse signals},
number = {11},
pages = {5389--5402},
title = {{Optimal two-stage search for sparse targets using convex criteria}},
volume = {56},
year = {2008}
}
@article{Belkin2010,
archivePrefix = {arXiv},
arxivId = {arXiv:0907.1054v2},
author = {Belkin, Mikhail and Sinha, Kaushik},
eprint = {arXiv:0907.1054v2},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Belkin, Sinha/2010 - Toward Learning Gaussian Mixtures with Arbitrary Separation.pdf:pdf},
journal = {Conference On Learning Theory (COLT)},
title = {{Toward Learning Gaussian Mixtures with Arbitrary Separation.}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Toward+Learning+Gaussian+Mixtures+with+Arbitrary+Separation{\#}0},
year = {2010}
}
@article{Keriven2016,
archivePrefix = {arXiv},
arxivId = {arXiv:0907.1054v2},
author = {Keriven, N. and Bourrier, A. and Gribonval, R. and P\'erez, P.},
journal = {arXiv:1606.02838},
title = {Sketching for Large-Scale Learning of Mixture Models},
year = {2016}
}
@article{Belkin2010a,
archivePrefix = {arXiv},
arxivId = {arXiv:1004.4864v1},
author = {Belkin, Mikhail and Sinha, Kaushik},
doi = {10.1109/FOCS.2010.16},
eprint = {arXiv:1004.4864v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Belkin, Sinha/2010 - Polynomial learning of distribution families.pdf:pdf},
isbn = {978-1-4244-8525-3},
journal = {IEEE 51st Annual Symposium on Foundations of Computer Science},
pages = {103--112},
publisher = {Ieee},
title = {{Polynomial learning of distribution families}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5670944},
year = {2010}
}
@article{Bertin2011,
author = {Bertin, Karine and {Le Pennec}, Erwan and Rivoirard, Vincent},
doi = {10.1214/09-AIHP351},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bertin, Le Pennec, Rivoirard/2011 - Adaptive Dantzig density estimation.pdf:pdf},
issn = {0246-0203},
journal = {Annales de l'Institut Henri Poincar{\'{e}}, Probabilit{\'{e}}s et Statistiques},
keywords = {62G05,62G07,62G20,Calibration,Concentration in,calibration,concentration inequalities,dantzig estimate,density estimation,dictionary,lasso estimate,oracle inequalities,sparsity},
number = {1},
pages = {43--74},
title = {{Adaptive Dantzig density estimation}},
url = {http://projecteuclid.org/euclid.aihp/1294170229},
volume = {47},
year = {2011}
}
@article{Blanchard2011,
annote = {- how to interpret the existing theoretical guarantees; especially RIP








- constants of proportional growth of k/n (oversampling rate) and n/N (undersampling rate) define a space for asymptotic analysis; 








- bounds for gaussian matrices








- separation LRIP URIP








- explicit asymptotic bounds, with exponential convergence in probability; proof using control of eigvalues Wishart matrices X{\^{}}TX and union bounds, using sharp bounds on the tails of the proba density function of the eigvalues (argh)
- no tighter bounds can be achieved in this asymptotic framework








- BRINGS A UNIFYING FRAMEWORK FOR COMPARISON OF CS RESULTS (often difficult to compare depending of the pov)
- region of the space where reconstruction is guaranteed (in proba)








- here only gaussian matrices








- incorporation in this framework of bounds coming from three distincts analysis: eigvalues and RIP, geometric functional analysis, convex polytope








- two step analysis: which condition is required (on RIP for instance) to have reconstruction guarantees, then use previous results on gaussian matrices to see on which region of the phase space this is verified








- Donoho polytope {\textgreater} Rudelson and Vershynin geometrical functional analysis {\textgreater} Foucart and Lai RIP








- not much discussion on these results; further examine the implications ? what are the other methods ?},
author = {Blanchard, JD and Cartis, Coralia and Tanner, Jared},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blanchard, Cartis, Tanner/2011 - Compressed sensing How sharp is the restricted isometry property.pdf:pdf},
journal = {SIAM Review},
keywords = {1,15a52,41a46,60f10,90c25,94a12,94a20,ams subject classifications,compressed sensing,consider the task of,convex,gaussian matrices,inner,introduction,matrices,measuring an unknown vector,phase transitions,primary,r n by taking,relaxation,restricted isometry property,secondary,singular values of random,sparse approximation,x},
number = {1},
pages = {105--125},
title = {{Compressed sensing: How sharp is the restricted isometry property?}},
url = {http://epubs.siam.org/doi/abs/10.1137/090748160},
volume = {53},
year = {2011}
}
@article{Blumensath2011,
archivePrefix = {arXiv},
arxivId = {arXiv:0911.3514v2},
author = {Blumensath, Thomas},
eprint = {arXiv:0911.3514v2},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blumensath/2011 - Sampling and reconstructing signals from a union of linear subspaces.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {7},
pages = {4660--4671},
title = {{Sampling and reconstructing signals from a union of linear subspaces}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5895053},
volume = {57},
year = {2011}
}
@article{Blumensath2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0805.0510v1},
author = {Blumensath, Thomas and Davies, Mike E.},
eprint = {arXiv:0805.0510v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blumensath, Davies/2009 - Iterative hard thresholding for compressed sensing.pdf:pdf},
journal = {Applied and Computational Harmonic Analysis},
number = {3},
pages = {265--274},
title = {{Iterative hard thresholding for compressed sensing}},
url = {http://www.sciencedirect.com/science/article/pii/S1063520309000384},
volume = {27},
year = {2009}
}
@article{Blumensath2009a,
author = {Blumensath, Thomas and Davies, Mike E.},
doi = {10.1109/TIT.2009.2013003},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blumensath, Davies/2009 - Sampling theorems for signals from the union of finite-dimensional linear subspaces.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {1872--1882},
title = {{Sampling theorems for signals from the union of finite-dimensional linear subspaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4802322 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4802322},
volume = {55},
year = {2009}
}
@inproceedings{Blumensath2008a,
abstract = {In this paper the linear sparse signal model is extended to allow more general, non-linear relationships and more general measures of approximation error. A greedy gradient based strategy is presented$\backslash$r$\backslash$nto estimate the sparse coefficients. This algorithm can be$\backslash$r$\backslash$nunderstood as a generalisation of the recently introduced Gradient Pursuit framework. Using the presented approach with the traditional linear model but with a different cost function is shown to outperform OMP in terms of recovery of the original sparse coefficients. A second set of experiments then shows that for the nonlinear$\backslash$r$\backslash$nmodel studied and for highly sparse signals, recovery is still possible in at least a percentage of cases.},
author = {Blumensath, Thomas and Davies, Mike E.},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blumensath, Davies/2008 - Gradient pursuit for non-linear sparse signal modelling.pdf:pdf},
issn = {22195491},
title = {{Gradient pursuit for non-linear sparse signal modelling}},
year = {2008}
}
@article{Blumensath2008,
author = {Blumensath, Thomas and Davies, Mike E.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Blumensath, Davies/2008 - Iterative thresholding for sparse approximations.pdf:pdf},
journal = {Journal of Fourier Analysis and Applications},
keywords = {and phrases,isation,iterative thresholding,sparse approximations,subset selection,ℓ 0 regular-},
number = {5-6},
pages = {629--654},
title = {{Iterative thresholding for sparse approximations}},
url = {http://link.springer.com/article/10.1007/s00041-008-9035-z},
volume = {14},
year = {2008}
}
@inproceedings{Bo2009,
abstract = {To determine the breeding system of Lycium cestroides, several treatments were performed: self-, cross-, and geitonogamous pollinations, autonomous self-pollination, and a control (flowers exposed to natural pollinators). Production and quality of fruits and seeds as well as pollen tube growth were evaluated for each treatment. Experimental pollinations indicate that L. cestroides is self-incompatible since fruits were obtained only under cross-, and open-pollination treatments. However, in self- and geitonogamous hand pollinations, as well as in autonomous self-pollination, pollen tubes developed successfully and reached the ovules. The speed of pollen tube growth did not differ significantly among the different hand-pollinated flowers (cross-, self-, and geitonogamous). These facts indicate the presence of an ovarian self-incompatibility system. Significant differences were observed in fruit set, fruit size, and seed number per fruit between cross-pollinated flowers and open pollination (control). These results could be explained in terms of quantity and quality of pollination in each case},
author = {Bo, Liefeng and Sminchisescu, Cristian},
booktitle = {Advances in Neural Information Processing System (NIPS)},
doi = {10.1002/hrdq},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bo, Sminchisescu/2009 - Efficient Match Kernels between Sets of Features for Visual Recognition.pdf:pdf},
isbn = {9781615679119},
title = {{Efficient Match Kernels between Sets of Features for Visual Recognition}},
url = {http://www.robots.ox.ac.uk/{~}vgg/rg/papers/efficientmatch.pdf},
year = {2009}
}
@article{Bourrier2014,
author = {Bourrier, Anthony and Davies, Mike E. and Peleg, Tomer},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bourrier, Davies, Peleg/2014 - Fundamental performance limits for ideal decoders in high-dimensional linear inverse problems.pdf:pdf;:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bourrier, Davies, Peleg/2014 - Fundamental performance limits for ideal decoders in high-dimensional linear inverse problems(2).pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {12},
pages = {7928--7946},
title = {{Fundamental performance limits for ideal decoders in high-dimensional linear inverse problems}},
url = {http://arxiv.org/abs/1311.6239},
volume = {60},
year = {2014}
}
@incollection{Bourrier2015,
abstract = {When fitting a probability model to voluminous data, memory and computational time can become prohibitive. In this paper, we propose a framework aimed at fitting a mixture of isotropic Gaussians to data vectors by computing a low-dimensional sketch of the data. The sketch represents empirical moments of the underlying probability distribution. Deriving a reconstruction algorithm by analogy with compressive sensing, we experimentally show that it is possible to precisely estimate the mixture parameters provided that the sketch is large enough. Our algorithm provides good reconstruction and scales to higher dimensions than previous probability mixture estimation algorithms, while consuming less memory in the case of numerous data. It also provides a privacy-preserving data analysis tool, since the sketch doesn't disclose information about individual datum it is based on.},
author = {Bourrier, Anthony and Gribonval, Remi and Perez, Patrick},
booktitle = {Compressed Sensing and its Applications - MATHEON Workshop 2013},
doi = {10.1109/ICASSP.2013.6638821},
editor = {Boche, Haulger and Calderbank, Robert and Kutyniok, Gitta and Vybiral, Jan},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bourrier, Gribonval, Perez/2015 - Compressive Gaussian Mixture estimation.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Gaussian mixture estimation,compressive learning,compressive sensing,database sketch},
pages = {6024--6028},
publisher = {Birkh{\"{a}}user Basel},
title = {{Compressive Gaussian Mixture estimation}},
year = {2015}
}
@inproceedings{Bourrier2013,
annote = {Directly compress the distribution through compressed-sensing based paradigms 
Empirical moments of (sometimes random) non-linear functions play the role of usual random linear projections: here characteristic function of the distribution at various frequencies
The simple form of the distribution (isotropic Gaussian mixture of limited size) plays the role of the sparsity for reconstruction
































































































































Reconstruction is achieved through a IHT-like algo, but continuous: the support contains the means of the current estimated gaussians, it is iteratively extended with possible candidates and then reduced to only {\$}K{\$} elements through a projection and hard thresholding
Use: memory compression, confidentiality
Future work: non-isotropic gaussians},
author = {Bourrier, Anthony and Gribonval, R{\'{e}}mi and P{\'{e}}rez, Patrick},
booktitle = {IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bourrier, Gribonval, P{\'{e}}rez/2013 - Compressive gaussian mixture estimation.pdf:pdf},
keywords = {Gaussian mixture estimation,compressive learning.,compressive sensing,database sketch},
pages = {6024--6028},
title = {{Compressive gaussian mixture estimation}},
url = {http://www.google.com/patents/US3157629 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6638821},
year = {2013}
}
@article{Bunea2010,
archivePrefix = {arXiv},
arxivId = {arXiv:0901.2044v2},
author = {Bunea, Florentina and Tsybakov, Alexandre B. and Wegkamp, Marten H. and Barbu, Adrian},
doi = {10.1214/09-AOS790},
eprint = {arXiv:0901.2044v2},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Bunea et al/2010 - SPADES and mixture models.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {62C20,62G05,62G08,62G20,Adaptive estimation,a},
number = {4},
pages = {2525--2558},
title = {{SPADES and mixture models}},
url = {http://projecteuclid.org/euclid.aos/1278861256},
volume = {38},
year = {2010}
}
@techreport{Calderbank2009,
annote = {Direct learning on compressed data. Theoretical results on performance and separability in the measurement domain.},
author = {Calderbank, Robert and Jafarpour, Sina and Schapire, Robert},
booktitle = {Preprint},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Calderbank, Jafarpour, Schapire/2009 - Compressed learning Universal sparse dimensionality reduction and learning in the measurement domain.pdf:pdf},
title = {{Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain}},
url = {http://dsp.rice.edu/files/cs/cl.pdf},
year = {2009}
}
@article{Candes2008,
author = {Candes, Emmanuel},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes/2008 - The restricted isometry property and its implications for compressed sensing.pdf:pdf},
journal = {Comptes Rendus Mathematique},
number = {9-10},
pages = {589--592},
title = {{The restricted isometry property and its implications for compressed sensing}},
url = {http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
volume = {346},
year = {2008}
}
@article{Candes2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1005.2613v3},
author = {Candes, Emmanuel and Eldar, Yonina C. and Needell, Deanna and Randall, Paige},
eprint = {arXiv:1005.2613v3},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes et al/2011 - Compressed sensing with coherent and redundant dictionaries.pdf:pdf},
journal = {Applied and Computational Harmonic Analysis},
number = {1},
pages = {59--73},
title = {{Compressed sensing with coherent and redundant dictionaries}},
url = {http://www.sciencedirect.com/science/article/pii/S1063520310001156},
volume = {31},
year = {2011}
}
@article{Candes2011a,
abstract = {This paper presents several novel theoretical results regarding the recovery of a low-rank matrix from just a few measurements consisting of linear combinations of the matrix entries. We show that properly constrained nuclear-norm minimization stably recovers a low-rank matrix from a constant number of noisy measurements per degree of freedom; this seems to be the first result of this nature. Further, with high probability, the recovery error from noisy data is within a constant of three targets: 1) the minimax risk, 2) an {\&}{\#}x201C;oracle{\&}{\#}x201D; error that would be available if the column space of the matrix were known, and 3) a more adaptive {\&}{\#}x201C;oracle{\&}{\#}x201D; error which would be available with the knowledge of the column space corresponding to the part of the matrix that stands above the noise. Lastly, the error bounds regarding low-rank matrices are extended to provide an error bound when the matrix has full rank with decaying singular values. The analysis in this paper is based on the restricted isometry property (RIP).},
archivePrefix = {arXiv},
arxivId = {1001.0339},
author = {Candes, Emmanuel and Plan, Yaniv},
doi = {10.1109/TIT.2011.2111771},
eprint = {1001.0339},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes, Plan/2011 - Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Convex optimization,Dantzig selector,matrix completion,norm of random matrices,oracle inequalities and semidefinite programming},
number = {4},
pages = {2342--2359},
title = {{Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements}},
volume = {57},
year = {2011}
}
@article{Candes2006,
author = {Candes, Emmanuel and Romberg, Justin K. and Tao, Terence},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes, Romberg, Tao/2006 - Stable signal recovery from incomplete and inaccurate measurements.pdf:pdf},
journal = {Communications on Pure and Applied Mathematics},
keywords = {1 -minimization,a national science foundation,acknowledgments,and by an alfred,basis pursuit,c,dms 01-40698,e,frg,grant,is partially supported by,is supported by national,j,p,r,restricted orthonormality,singular,sloan fellowship,sparsity,values of random matrices},
number = {8},
pages = {1207--1223},
title = {{Stable signal recovery from incomplete and inaccurate measurements}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cpa.20124/abstract},
volume = {59},
year = {2006}
}
@article{Candes2004,
archivePrefix = {arXiv},
arxivId = {math/0409186},
author = {Candes, Emmanuel and Romberg, Justin K. and Tao, Terence},
eprint = {0409186},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes, Romberg, Tao/2006 - Robust uncertainty principles Exact signal reconstruction from highly incomplete frequency information.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {480----509},
primaryClass = {math},
title = {{Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information}},
url = {http://arxiv.org/abs/math/0409186v1 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1580791},
volume = {52},
year = {2006}
}
@article{Candes2006a,
author = {Candes, Emmanuel and Tao, Terence},
doi = {10.1109/TIT.2006.885507},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes, Tao/2006 - Near-optimal signal recovery from random projections Universal encoding strategies.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {acknowledgments,c,concentration of measure,convex optimization,dom projections,duality in optimization,e,is partially supported by,linear programming,matrices,national science foundation grants,principle,ran-,random matrices,signal recovery,singular values of random,sparsity,trigonometric expansions,uncertainty},
number = {12},
pages = {5406--5425},
title = {{Near-optimal signal recovery from random projections: Universal encoding strategies?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4016283 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4016283},
volume = {52},
year = {2006}
}
@article{Candes2005,
author = {Candes, Emmanuel and Tao, Terence},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Candes, Tao/2005 - Decoding by linear programming.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {1 minimization,basis pursuit,decoding of,determined systems,duality in optimization,gaussian random matrices,gramming,linear codes,linear pro-,principal angles,random,restricted orthonormality,singular,sparse solutions to under-,values of random matrices},
number = {12},
pages = {4203--4215},
title = {{Decoding by linear programming}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1542412},
volume = {51},
year = {2005}
}
@article{Cappe2009,
abstract = {In this contribution, we propose a generic online (also sometimes called adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm applicable to latent variable models of independent observations. Compared to the algorithm of Titterington (1984), this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e., that of the maximum likelihood estimator. In addition, the proposed approach is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model.},
archivePrefix = {arXiv},
arxivId = {0712.4273},
author = {Capp{\'{e}}, Olivier and Moulines, Eric},
doi = {10.1111/j.1467-9868.2009.00698.x},
eprint = {0712.4273},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Capp{\'{e}}, Moulines/2009 - Online EM Algorithm for Latent Data Models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society},
keywords = {adaptive algorithms,expectation-maximisation,latent data models,mixture of regressions,online estima-,polyak-ruppert averaging,stochastic approximation,tion},
number = {3},
pages = {593--613},
title = {{Online EM Algorithm for Latent Data Models}},
url = {http://arxiv.org/abs/0712.4273},
volume = {71},
year = {2009}
}
@article{Carrasco2002,
annote = {Use the characteristic function as an alternative to the ML},
author = {Carrasco, Marine and Florens, Jean-Pierre},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Carrasco, Florens/2002 - Efficient GMM estimation using the empirical characteristic function.pdf:pdf;:C$\backslash$:/Users/nkeriven/Documents/mendeley/Carrasco, Florens/2002 - Efficient GMM estimation using the empirical characteristic function(2).pdf:pdf},
journal = {IDEI Working Paper},
keywords = {2000,a former version of,and josef,and paris-berlin,convolution e ciency gmm,econometrics,francis kramarz,mixture stable distribution,nour meddahi,participants of the conferences,perktold for helpful comments,society world congress,the authors are grateful,they also thank the,this paper has circu-,to christian gourieroux},
number = {2000},
title = {{Efficient GMM estimation using the empirical characteristic function}},
url = {http://neeo.univ-tlse1.fr/919/1/efficientgmm.pdf},
year = {2002}
}
@article{Castro2012,
annote = {Basis pursuit (with total variation norm) for discrete measure recovery with finite number of matching moments
































for Fourier coeff: Beurling minimization
































reconstruction understood in term of sparsity and finiteness of support
































fairly technical},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4951v2},
author = {Castro, Yohann De and Gamboa, Fabrice},
eprint = {arXiv:1103.4951v2},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Castro, Gamboa/2012 - Exact reconstruction using Beurling minimal extrapolation.pdf:pdf},
journal = {Journal of Mathematical Analysis and Applications},
keywords = {and phrases,basis pursuit,beurling minimal extrapolation,compressed sensing,con-},
pages = {336--354},
title = {{Exact reconstruction using Beurling minimal extrapolation}},
url = {http://www.sciencedirect.com/science/article/pii/S0022247X12003952},
volume = {395},
year = {2012}
}
@article{Cohen2009,
author = {Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cohen, Dahmen, DeVore/2009 - Compressed sensing and best k-term approximation.pdf:pdf},
journal = {Journal of the American mathematical Society},
keywords = {1 -minimization,and phrases,best k -term approximation,coders,compressed sensing,gauss-,gelfand width,ian and bernoulli ensembles,instance optimal de-,instance optimality in,mixed instance optimality,null space property,random matrices,restricted isometry property},
number = {1},
pages = {211--231},
title = {{Compressed sensing and best k-term approximation}},
url = {http://www.ams.org/jams/2009-22-01/S0894-0347-08-00610-3/},
volume = {22},
year = {2009}
}
@article{Cohn1996,
abstract = {For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.},
archivePrefix = {arXiv},
arxivId = {cs/9603104},
author = {Cohn, David A. and Ghahramani, Zoubin and Jordan, Michael I.},
doi = {10.1613/jair.295},
eprint = {9603104},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cohn, Ghahramani, Jordan/1996 - Active Learning with Statistical Models.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {129--145},
primaryClass = {cs},
title = {{Active Learning with Statistical Models}},
url = {http://arxiv.org/abs/cs/9603104},
volume = {4},
year = {1996}
}
@article{Cormode2009,
author = {Cormode, Graham and Hadjieleftheriou, Marios},
doi = {10.1007/s00778-009-0172-z},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cormode, Hadjieleftheriou/2009 - Methods for finding frequent items in data streams.pdf:pdf},
issn = {1066-8888},
journal = {The VLDB Journal},
number = {1},
pages = {3--20},
title = {{Methods for finding frequent items in data streams}},
url = {http://link.springer.com/10.1007/s00778-009-0172-z},
volume = {19},
year = {2009}
}
@inproceedings{Cormode2004,
author = {Cormode, Graham and Korn, Flip and Muthukrishnan, S. and Divesh, Srivastava},
booktitle = {International Conference on Management of Data},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cormode et al/2004 - Diamond in the rough Finding hierarchical heavy hitters in multi-dimensional data.pdf:pdf},
isbn = {1581138598},
pages = {155--166},
title = {{Diamond in the rough: Finding hierarchical heavy hitters in multi-dimensional data}},
url = {http://dl.acm.org/citation.cfm?id=1007588},
year = {2004}
}
@article{Cormode2005,
author = {Cormode, Graham and Muthukrishnan, S.},
doi = {10.1016/j.jalgor.2003.12.001},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cormode, Muthukrishnan/2005 - An improved data stream summary the count-min sketch and its applications.pdf:pdf},
issn = {01966774},
journal = {Journal of Algorithms},
number = {1},
pages = {58--75},
title = {{An improved data stream summary: the count-min sketch and its applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0196677403001913},
volume = {55},
year = {2005}
}
@book{Cover2006,
author = {Cover, Thomas M. and Thomas, Joy A.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cover, Thomas/2006 - Elements of information theory.pdf:pdf},
isbn = {0070050236},
publisher = {Wiley-Interscience},
title = {{Elements of information theory}},
year = {2006}
}
@article{Cucker2002,
abstract = {In this report the authors study the relationship between approximation and learning, emphasizing the primary role of sampling (inductive inference). Tools and ideas from linear algebra, probability theory and numerical analysis (least squares algorithm) are widely used. As the authors say, ``practical results are not the goal of this paper. Understanding is.'' With this purpose, they illustrate their statements with several examples. The main setting is the following: the existence of an ``unknown'' function fcolon ,X to Y and a probability measure $\rho$ allowing one to randomly draw points in X times Y is assumed, such that for x in X the expected value of a randomly drawn point y in Y is f(x). The (least squares) error of f is defined by scr E(f)=intX times Y (f(x)-y) 2 d$\rho$. The main goal is to find f minimizing scr E(f), where f is taken from a subfamily scr H of continuous functions with the sup-norm on X. Along with the optimizer fscr H of the problem above, the empirical target function fz is considered, minimizing the discrete least squares error for a given sample z in (X times Y) m. Then the error scr E(fz) decomposes as a sum of the sample error (which measures how good fz is as an approximation of fscr H), studied in detail in Chapter I, and the approximation error, which depends only on scr H and $\rho$ (Chapter II). Several estimations on both errors in terms of covering numbers are provided. Also, the bias-variance trade-off problem is discussed (for larger families scr H the approximation errors decrease, but sample errors increase, and vice-versa). Chapter III is devoted to the study of a particular choice of the families scr H, which allows the analysis of algorithms from the point of view of reproducing kernel Hilbert spaces.},
author = {Cucker, Felipe and Smale, Steve},
doi = {10.1090/S0273-0979-01-00923-5},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Cucker, Smale/2002 - On the mathematical foundations of learning.pdf:pdf},
isbn = {0884-8289},
issn = {02730979},
journal = {Bulletin of the American Mathematical Society},
number = {1},
pages = {1--49},
pmid = {18382608},
title = {{On the mathematical foundations of learning}},
volume = {39},
year = {2002}
}
@article{Dasgupta1999,
annote = {compression of individual vectors, keep good properties wrt gaussian centers. Random subspace then EM
















EM : curse of dimensionality (oh rly ?)
algo in nK²
theoretical result, user-defined precision (not proved before)
projection of the Gaussian in low-d subspace
















use of Gaussian unit in term of distance from the center, chi2 law ! (mine) -{\textgreater} separability of gaussians defined in terms of $\backslash$sigma$\backslash$sqrt{\{}n{\}}
















dim reduction: lindenstrauss lemma: reduce dim of individual vectors without perturbing distances. Here, use of random subspace to reduce the dim many more, BY PERTURBING the pariwise distance (without, if possible, perturbing distance between centers)},
author = {Dasgupta, Sanjoy},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Dasgupta/1999 - Learning mixtures of Gaussians.pdf:pdf},
journal = {Foundations of Computer Science},
number = {May},
title = {{Learning mixtures of Gaussians}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=814639},
year = {1999}
}
@article{Do2011,
archivePrefix = {arXiv},
arxivId = {1106.5037v1},
author = {Do, Thong T. and Gan, Lu and Nguyen, Nam H. and Tran, Trac D.},
doi = {10.1007/s11517-011-0832-1},
eprint = {1106.5037v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Do et al/2011 - Fast and Efficient Compressive Sensing using Structurally Random Matrices.pdf:pdf},
isbn = {0140-0118},
issn = {01400118},
journal = {IEEE Transactions on Signal Processing},
pmid = {21947867},
title = {{Fast and Efficient Compressive Sensing using Structurally Random Matrices}},
volume = {30},
year = {2011}
}
@article{Donoho2006,
author = {Donoho, David L.},
doi = {10.1109/TIT.2006.871582},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Donoho/2006 - Compressed sensing.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {1289--1306},
title = {{Compressed sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1614066},
volume = {52},
year = {2006}
}
@article{Donoho2012,
author = {Donoho, David L. and Tsaig, Yaakov},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Donoho, Tsaig/2012 - Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {1 minimization,compressed sensing,decoding error-correcting codes,false alarm rate,false discovery rate,gaussian approximation,interference,iterative decoding,large-system limit,mimo channel,mutual access,phase transition,random matrix theory,sparse overcomplete representation,stepwise regression,successive interference cancellation,thresholding},
number = {2},
pages = {1--39},
title = {{Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6145475},
volume = {58},
year = {2012}
}
@techreport{Duchi,
author = {Duchi, John},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Duchi/2007 - Derivations for Linear Algebra and Optimization.pdf:pdf},
title = {{Derivations for Linear Algebra and Optimization}},
year = {2007}
}
@article{Fedotov2003,
author = {Fedotov, Alexei A and Harremo{\"{e}}s, Peter and Tops{\o}e, Flemming},
doi = {10.1109/TIT.2003.811927},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Fedotov, Harremo{\"{e}}s, Tops{\o}e/2003 - Refinements of Pinsker's Inequality.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {1491--1498},
title = {{Refinements of Pinsker's Inequality}},
volume = {49},
year = {2003}
}
@article{Feuerverger1981,
author = {Feuerverger, Andrey and McDunnough, Philip},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Feuerverger, McDunnough/1981 - On Some Fourier methods for Inference.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {374},
pages = {379--387},
title = {{On Some Fourier methods for Inference}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1981.10477657},
volume = {76},
year = {1981}
}
@article{Fischer2012,
abstract = {This is a note of purely didactical purpose as the proof of the Jordan measure decomposition is often omitted in the related literature. Elementary proofs are provided for the existence, the uniqueness, and the minimality property of the Jordan decomposition for a finite signed measure.},
archivePrefix = {arXiv},
arxivId = {1206.5449},
author = {Fischer, Tom},
eprint = {1206.5449},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Fischer/2012 - Existence, uniqueness, and minimality of the Jordan measure decomposition.pdf:pdf},
journal = {arXiv:1206.5449},
title = {{Existence, uniqueness, and minimality of the Jordan measure decomposition}},
url = {http://arxiv.org/abs/1206.5449},
year = {2012}
}
@book{Foucart2013,
address = {New York, NY},
author = {Foucart, Simon and Rauhut, Holger},
doi = {10.1007/978-0-8176-4948-7},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Foucart, Rauhut/2013 - A Mathematical Introduction to Compressive Sensing.pdf:pdf},
isbn = {978-0-8176-4947-0},
publisher = {Springer New York},
series = {Applied and Numerical Harmonic Analysis},
title = {{A Mathematical Introduction to Compressive Sensing}},
url = {http://link.springer.com/10.1007/978-0-8176-4948-7},
year = {2013}
}
@inproceedings{Fradkin2003,
address = {New York, New York, USA},
author = {Fradkin, Dmitriy and Madigan, David},
booktitle = {International Conference on Knowledge discovery and data mining (KDD)},
doi = {10.1145/956804.956812},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Fradkin, Madigan/2003 - Experiments with random projections for machine learning.pdf:pdf},
isbn = {1581137370},
pages = {517--522},
publisher = {ACM Press},
title = {{Experiments with random projections for machine learning}},
url = {http://portal.acm.org/citation.cfm?doid=956750.956812},
year = {2003}
}
@inproceedings{Fukumizu2008,
abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
author = {Fukumizu, K. and Gretton, A. and Sun, X. and Sch{\"{o}}lkopf, B.},
booktitle = {Advances in Neural Information Processing System (NIPS)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Fukumizu et al/2007 - Kernel Measures of Conditional Dependence.pdf:pdf},
isbn = {978-1-605-60352-0},
keywords = {Brain Computer Interfaces,Computational,Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
title = {{Kernel Measures of Conditional Dependence}},
url = {http://eprints.pascal-network.org/archive/00004334/},
year = {2007}
}
@article{Fukunaga1983,
abstract = {Given a general n-dimensional bimodal Gaussian mixture, this paper shows how unknown parameters may be found by the method of moments. Three cases are considered-equal modal probabilities, known but not necessarily equal probabilities, and all parameters unknown. The solution involves sample moments no higher than fourth order. For Gaussian mixtures where the number of modes is unknown, fourth-order moments can be used to count them, provided all modes have the same covariance matrix, and their multiplicity is not greater than data dimensionality. Examples of mode-counting and the determination of bimodal parameters are included.},
author = {Fukunaga, Keinosuke and Flick, Thomas E.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Fukunaga, Flick/1983 - Estimation of the parameters of a gaussian mixture using the method of moments.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {410--416},
pmid = {21869125},
title = {{Estimation of the parameters of a gaussian mixture using the method of moments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869125},
volume = {5},
year = {1983}
}
@inproceedings{Gilbert2002,
author = {Gilbert, Anna C. and Kotidis, Yannis and Muthukrishnan, S. and Strauss, Martin J.},
booktitle = {International Conference on Very Large Data Bases (VLBD)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Gilbert et al/2002 - How to summarize the universe Dynamic maintenance of quantiles.pdf:pdf},
pages = {454--465},
title = {{How to summarize the universe: Dynamic maintenance of quantiles}},
url = {http://dl.acm.org/citation.cfm?id=1287409},
year = {2002}
}
@article{Giryes2015,
abstract = {Three important properties of a classification machinery are: (i) the system preserves the important information of the input data; (ii) the training examples convey information for unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are inherited by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure; and provide bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
annote = {DNN with random gaussian weights: embedding of input data

preservation of metrics expressed as a quasi-isometry (ie "additive RIP") on a manifold K

most of the results employs the gaussian width of K. To control a succession of layers, the growth of the covering numbers of K is controlled at each layer.

preservation of metrics also expressed as an appropriate shrinkage of the angles between input points. Though not analyzed, the role of training a DNN could be to select an appropriate origin to treat the angles: this results in a metric learning algorithm, which can be related to eg kernel methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08291v1},
author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M},
eprint = {arXiv:1504.08291v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Giryes, Sapiro, Bronstein/2015 - Deep Neural Networks with Random Gaussian Weights A Universal Classification Strategy.pdf:pdf},
journal = {arXiv:1504.08291},
pages = {1--9},
title = {{Deep Neural Networks with Random Gaussian Weights : A Universal Classification Strategy ?}},
year = {2015}
}
@inproceedings{Gretton2008,
abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
archivePrefix = {arXiv},
arxivId = {0805.2368},
author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte and Scholkopf, Bernhard and Smola, Alexander J.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {0805.2368},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Gretton et al/2006 - A Kernel Method for the Two-Sample Problem.pdf:pdf},
isbn = {0-262-19568-2},
issn = {1049-5258},
title = {{A Kernel Method for the Two-Sample Problem}},
url = {http://discovery.ucl.ac.uk/1334333/},
year = {2006}
}
@inproceedings{Gribonval1996,
abstract = {Sound recordings include transients and sustained parts. Their analysis with a basis expansion is not rich enough to represent efficiently all such components. Pursuit algorithms choose the decomposition vectors depending upon the signal properties. The dictionary among which these vectors are selected is much larger than a basis. Matching pursuit is fast to compute, but can provide coarse representations. Basis pursuit gives a better representation but is very expensive in terms of calculation time. This paper develops a high resolution matching pursuit: it is a fast, high time-resolution, time-frequency analysis algorithm, that makes it likely to be used far musical applications View full abstract},
author = {Gribonval, R{\'{e}}mi and Bacry, Emmanuel and Mallat, St{\'{e}}phane and Depalle, Philippe and Rodet, Xavier},
booktitle = {IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis},
doi = {10.1109/TFSA.1996.546702},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Gribonval et al/1996 - Analysis of sound signals with high resolution matching pursuit.pdf:pdf},
isbn = {0-7803-3512-0},
pages = {125--128},
title = {{Analysis of sound signals with high resolution matching pursuit}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=546702},
year = {1996}
}
@book{Hall2005,
author = {Hall, Alastair R.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Hall/2005 - Generalized method of moments.pdf:pdf},
isbn = {0198775210},
title = {{Generalized method of moments}},
url = {http://personalpages.manchester.ac.uk/staff/Alastair.Hall/GMM{\_}EQF{\_}100309.pdf},
year = {2005}
}
@article{Haupt2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1001.5311v2},
author = {Haupt, Jarvis and Castro, Rui and Nowak, Robert and May, S T},
eprint = {arXiv:1001.5311v2},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Haupt et al/2011 - Distilled Sensing Adaptive Sampling for Sparse Detection and Estimation.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {9},
pages = {6222--6235},
title = {{Distilled Sensing : Adaptive Sampling for Sparse Detection and Estimation}},
volume = {57},
year = {2011}
}
@article{Hofmann2006,
abstract = {We review recent methods for learning with positive definite kernels. All these methods formulate learning and estimation problems as linear tasks in a reproducing kernel Hilbert space (RKHS) associated with a kernel. We cover a wide range of methods, ranging from simple classifiers to sophisti- cated methods for estimation with structured data.},
author = {Hofmann, T and Sch{\"{o}}lkopf, B and Smola, a J},
doi = {10.1.1.88.7691},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Hofmann, Sch{\"{o}}lkopf, Smola/2008 - Kernel methods in machine learning.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {machine learning,reproducing kernels,support vector machines},
number = {3},
pages = {1171--1220},
title = {{Kernel methods in machine learning}},
url = {papers2://publication/uuid/35786BBA-E664-4788-A4CE-52FA00239A0B},
volume = {36},
year = {2008}
}
@inproceedings{Hsu2013,
annote = {Isotropic gaussian, moment method
































Low-order observable moment, but still order 3. Non-linear. Has a closed form expression in terms of the parameters of the Gaussians, and so is sufficient for recovering them.
































Related to multi view methods (anandkumar)},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5766v4},
author = {Hsu, Daniel and Kakade, Sham M.},
booktitle = {Conference on Innovations in Theoretical Computer Science},
eprint = {arXiv:1206.5766v4},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Hsu, Kakade/2013 - Learning mixtures of spherical gaussians moment methods and spectral decompositions.pdf:pdf},
pages = {1--29},
title = {{Learning mixtures of spherical gaussians: moment methods and spectral decompositions}},
url = {http://dl.acm.org/citation.cfm?id=2422439},
year = {2013}
}
@inproceedings{Jain2011,
author = {Jain, Prateek and Tewari, Ambuj and Dhillon, Inderjit S.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Jain, Tewari, Dhillon/2011 - Orthogonal matching pursuit with replacement.pdf:pdf;:C$\backslash$:/Users/nkeriven/Documents/mendeley/Jain, Tewari, Dhillon/2011 - Orthogonal matching pursuit with replacement(2).pdf:pdf},
pages = {1215--1223},
title = {{Orthogonal matching pursuit with replacement}},
url = {http://papers.nips.cc/paper/4462-orthogonal-matching-pursuit-with-replacement},
year = {2011}
}
@inproceedings{Joshi2010,
abstract = {Starting with a similarity function between objects, it is possible to define a distance metric on pairs of objects, and more generally on probability distributions over them. These distance metrics have a deep basis in functional analysis, measure theory and geometric measure theory, and have a rich structure that includes an isometric embedding into a (possibly infinite dimensional) Hilbert space. They have recently been applied to numerous problems in machine learning and shape analysis. In this paper, we provide the first algorithmic analysis of these distance metrics. Our main contributions are as follows: (i) We present fast approximation algorithms for computing the kernel distance between two point sets P and Q that runs in near-linear time in the size of (P cup Q) (note that an explicit calculation would take quadratic time). (ii) We present polynomial-time algorithms for approximately minimizing the kernel distance under rigid transformation; they run in time O(n + poly(1/epsilon, log n)). (iii) We provide several general techniques for reducing complex objects to convenient sparse representations (specifically to point sets or sets of points sets) which approximately preserve the kernel distance. In particular, this allows us to reduce problems of computing the kernel distance between various types of objects such as curves, surfaces, and distributions to computing the kernel distance between point sets. These take advantage of the reproducing kernel Hilbert space and a new relation linking binary range spaces to continuous range spaces with bounded fat-shattering dimension.},
archivePrefix = {arXiv},
arxivId = {1001.0591},
author = {Joshi, Sarang and Kommaraju, Raj Varma and Phillips, Jeff M. and Venkatasubramanian, Suresh},
booktitle = {Proceedings of the twenty-seventh annual symposium on Computational geometry (SoCG)},
doi = {10.1145/1998196.1998204},
eprint = {1001.0591},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Joshi et al/2010 - Comparing Distributions and Shapes using the Kernel Distance.pdf:pdf},
isbn = {9781450306829},
pages = {47--56},
title = {{Comparing Distributions and Shapes using the Kernel Distance}},
url = {http://arxiv.org/abs/1001.0591},
year = {2010}
}
@inproceedings{Kanagawa2014,
author = {Kanagawa, Motonobu and Fukumizu, Kenji},
booktitle = {17th International Conference on Artificial Intelligence and Statistics},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Kanagawa, Fukumizu/2014 - Recovering Distributions from Gaussian RKHS Embeddings.pdf:pdf},
pages = {457--465},
title = {{Recovering Distributions from Gaussian RKHS Embeddings}},
volume = {33},
year = {2014}
}
@book{Lawson1995,
abstract = {An accessible text for the study of numerical methods for solving least squares problems remains an essential part of a scientific software foundation. This book has served this purpose well. Numerical analysts, statisticians, and engineers have developed techniques and nomenclature for the least squares problems of their own discipline. This well-organized presentation of the basic material needed for the solution of least squares problems can unify this divergence of methods. Mathematicians, practising engineers, and scientists will welcome its return to print. The material covered includes Householder and Givens orthogonal transformations, the QR and SVD decompositions, equality constraints, solutions in nonnegative variables, banded problems, and updating methods for sequential estimation. Both the theory and practical algorithms are included. The easily understood explanations and the appendix providing a review of basic linear algebra make the book accessible for the non-specialist.},
author = {Lawson, Charles L. and Hanson, Richard J.},
booktitle = {SIAM classics in applied mathematics},
doi = {10.2307/2005340},
isbn = {0898713560},
issn = {00255718},
pages = {337},
pmid = {1724298},
title = {{Solving least squares problems}},
url = {http://books.google.com/books?id=ROw4hU85nz8C},
volume = {15},
year = {1995}
}
@article{LeMagoarou2015,
abstract = {The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems.},
archivePrefix = {arXiv},
arxivId = {1506.07300},
author = {{Le Magoarou}, Luc and Gribonval, R{\'{e}}mi},
doi = {10.1109/JSTSP.2016.2543461},
eprint = {1506.07300},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Le Magoarou, Gribonval/2016 - Flexible Multi-layer Sparse Approximations of Matrices and Applications.pdf:pdf},
isbn = {2011277906},
journal = {IEEE Journal of Selected Topics in Signal Processing},
number = {4},
pages = {688--700},
title = {{Flexible Multi-layer Sparse Approximations of Matrices and Applications}},
url = {http://arxiv.org/abs/1506.07300$\backslash$nhttp://www.arxiv.org/pdf/1506.07300.pdf},
volume = {10},
year = {2016}
}
@article{Le2013,
abstract = {Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that computing the de- cision function is typically expensive, espe- cially at prediction time. In this paper, we overcome this difficulty by proposing Fast- food, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matri- ces when combined with diagonal Gaussian matrices exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks (Rahimi {\&} Recht, 2007) and thereby speeding up the computation for a large range of ker- nel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) com- putation and storage, without sacrificing ac- curacy. We prove that the approximation is unbiased and has low variance. Extensive ex- periments show that we achieve similar accu- racy to full kernel expansions and Random Kitchen Sinks while being 100x faster and us- ing 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applica- tions that have large training sets and/or re- quire real-time prediction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.3060v1},
author = {Le, Quoc and Sarl{\'{o}}s, Tam{\'{a}}s and Smola, Alex J},
eprint = {arXiv:1408.3060v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Le, Sarl{\'{o}}s, Smola/2013 - Fastfood — Approximating Kernel Expansions in Loglinear Time.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
number = {1},
pages = {1--29},
title = {{Fastfood — Approximating Kernel Expansions in Loglinear Time}},
url = {file:///Users/dsutherl/Dropbox/Papers2/Articles/2013/Le/Fastfood ? Approximating Kernel Expansions in Loglinear Time.pdf$\backslash$npapers2://publication/uuid/351C3D3A-5831-49E5-A48F-FD7C344C79D3},
volume = {28},
year = {2013}
}
@inproceedings{Mailhe2009,
author = {Mailh{\'{e}}, Boris and Gribonval, R{\'{e}}mi},
booktitle = {Actes du XXIIe colloque GRETSI},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Mailh{\'{e}}, Gribonval/2009 - LocOMP algorithme localement orthogonal pour l'approximation parcimonieuse rapide de signaux longs sur des dictionnaires locaux.pdf:pdf},
title = {{LocOMP: algorithme localement orthogonal pour l'approximation parcimonieuse rapide de signaux longs sur des dictionnaires locaux}},
url = {http://hal.inria.fr/hal-00481761/},
year = {2009}
}
@inproceedings{Maillard2009,
annote = {Compression of the basis of functions on which the signal is decomposed, through random linear combination. Application to a Least Square algo and theoretical results},
author = {Maillard, Oldaric-Ambrym and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information and Processing Systems (NIPS)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Maillard, Munos/2009 - Compressed Least-Squares Regression.pdf:pdf},
pages = {1--9},
title = {{Compressed Least-Squares Regression.}},
url = {https://papers.nips.cc/paper/3698-compressed-least-squares-regression.pdf},
year = {2009}
}
@article{Mallat1993,
abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
author = {Mallat, St{\'{e}}phane and Zhang, Zhifeng},
doi = {10.1109/78.258082},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Mallat, Zhang/1993 - Matching Pursuit With Time-Frequency Dictionaries.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions in Signal Processing},
number = {12},
pages = {3397--3415},
pmid = {20298943},
title = {{Matching Pursuit With Time-Frequency Dictionaries}},
volume = {41},
year = {1993}
}
@article{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Sch{\"{o}}lkopf, Bernhard},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Muandet et al/2012 - Learning from distributions via support measure machines.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://papers.nips.cc/paper/4825-learning-from-distributions-via-support-measure-machines},
volume = {25},
year = {2012}
}
@book{Munkres2000,
author = {Munkres, James R.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Munkres/2000 - Topology.pdf:pdf},
publisher = {Pearson, 2nd edition},
title = {{Topology}},
year = {2000}
}
@article{Nam2013,
abstract = {After a decade of extensive study of the sparse representation synthesis model, we can safely say that this is a mature and stable field, with clear theoretical foundations, and appealing applications. Alongside this approach, there is an analysis counterpart model, which, despite its similarity to the synthesis alternative, is markedly different. Surprisingly, the analysis model did not get a similar attention, and its understanding today is shallow and partial. In this paper we take a closer look at the analysis approach, better define it as a generative model for signals, and contrast it with the synthesis one. This work proposes effective pursuit methods that aim to solve inverse problems regularized with the analysis-model prior, accompanied by a preliminary theoretical study of their performance. We demonstrate the effectiveness of the analysis model in several experiments, and provide a detailed study of the model associated with the 2D finite difference analysis operator, a close cousin of the TV norm. ?? 2012 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1106.4987},
author = {Nam, Sangnam and Davies, Mike E. and Elad, Michael and Gribonval, R{\'{e}}mi},
doi = {10.1016/j.acha.2012.03.006},
eprint = {1106.4987},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Nam et al/2013 - The cosparse analysis model and algorithms.pdf:pdf},
isbn = {1063-5203},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Analysis,Compressed-sensing,Greedy algorithms,Pursuit algorithms,Sparse representations,Synthesis,Union of subspaces},
number = {1},
pages = {30--56},
title = {{The cosparse analysis model and algorithms}},
url = {http://dx.doi.org/10.1016/j.acha.2012.03.006},
volume = {34},
year = {2013}
}
@article{Needell2008,
abstract = {Compressive sampling offers a new paradigm for acquiring signals that are compressible with respect to an orthonormal basis. The major algorithmic challenge in compressive sampling is to approximate a compressible signal from noisy samples. This paper describes a new iterative recovery algorithm called CoSaMP that delivers the same guarantees as the best optimization-based approaches. Moreover, this algorithm offers rigorous bounds on computational cost and storage. It is likely to be extremely efficient for practical problems because it requires only matrix-vector multiplies with the sampling matrix. For many cases of interest, the running time is just O(N*log{\^{}}2(N)), where N is the length of the signal.},
archivePrefix = {arXiv},
arxivId = {0803.2392},
author = {Needell, Deanna and Tropp, Joel A.},
eprint = {0803.2392},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Needell, Tropp/2009 - CoSaMP Iterative signal recovery from incomplete and inaccurate samples.pdf:pdf},
journal = {Applied and Computational Harmonic Analysis},
keywords = {1 shields ave,algorithms,and computational mathematics,and phrases,approximation,basis pursuit,ca 95616,california at davis,california inst,compressed sensing,davis,dn is with the,dneedell,e-mail,edu,jat is with applied,math,mathematics dept,mc 217-50,orthogonal matching pur-,restricted isometry property,signal recovery,sparse approximation,suit,technology,ucdavis,uncertainty principle,univ},
month = {mar},
number = {3},
pages = {301--321},
title = {{CoSaMP: Iterative signal recovery from incomplete and inaccurate samples}},
url = {http://arxiv.org/abs/0803.2392},
volume = {26},
year = {2009}
}
@article{Oliva2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.04150v1},
author = {Oliva, Junier B. and Sutherland, Dougal J. and Schneider, Jeff},
eprint = {arXiv:1511.04150v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Oliva, Sutherland, Schneider/2015 - Deep Mean Maps.pdf:pdf},
journal = {arXiv:1511.04150},
title = {{Deep Mean Maps}},
year = {2015}
}
@inproceedings{Pati1993,
author = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
booktitle = {Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.1993.342465},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Pati, Rezaiifar, Krishnaprasad/1993 - Orthogonal matching pursuit recursive function approximation with applications to wavelet decomposition.pdf:pdf},
isbn = {0-8186-4120-7},
pages = {40--44},
publisher = {IEEE Comput. Soc. Press},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=342465},
year = {1993}
}
@inproceedings{Pilanci2012,
annote = {discrete measure recovery via moment matching
































new sparsity promoting convex penalty since l1 is constant: inverse infinite norm
































application to clustering GMM; just equivalent reformulation of loglike with sparsity penalty. Avoid choosing the K in advance},
author = {Pilanci, Mert and {El Ghaoui}, Laurent and Chandrasekaran, Venkat},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Pilanci, El Ghaoui, Chandrasekaran/2012 - Recovery of sparse probability measures via convex programming.pdf:pdf},
pages = {2429--2437},
title = {{Recovery of sparse probability measures via convex programming}},
url = {http://papers.nips.cc/paper/4504-recovery-of-sparse-probability-measures-via-convex-programming},
year = {2012}
}
@inproceedings{Puy2015,
author = {Puy, Gilles and Davies, Mike E. and Gribonval, R{\'{e}}mi},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Puy, Davies, Gribonval/2015 - Linear embeddings of low-dimensional subsets of a Hilbert space to R m.pdf:pdf},
isbn = {2011277906},
pages = {469--473},
title = {{Linear embeddings of low-dimensional subsets of a Hilbert space to R m}},
year = {2015}
}
@article{Rahimi2009,
author = {Rahimi, Ali and Recht, Benjamin},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Rahimi, Recht/2009 - Weighted sums of random kitchen sinks Replacing minimization with randomization in learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
number = {1},
pages = {1--8},
title = {{Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning}},
url = {http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning},
volume = {1},
year = {2009}
}
@article{Rahimi2007,
author = {Rahimi, Ali and Recht, Benjamin},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Rahimi, Recht/2007 - Random Features for Large Scale Kernel Machines.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
number = {1},
pages = {1--8},
title = {{Random Features for Large Scale Kernel Machines}},
year = {2007}
}
@techreport{Rauhut2012,
author = {Rauhut, Holger},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Rauhut/2012 - Compressive Sensing and structured random matrices.pdf:pdf},
keywords = {1 -minimization,15a12,15a60,15b02,15b19,15b52,42a05,42a61,46b09,46b10,ams classification,basis pursuit,bounded orthogonal systems,compressive sensing,condition numbers,inequalities,khintchine,partial random circulant matrix,random partial fourier matrix,structured random matrices},
pages = {1--94},
title = {{Compressive Sensing and structured random matrices}},
year = {2012}
}
@inproceedings{Reboredo2013,
abstract = {This paper derives fundamental limits associated with compressive classification of Gaussian mixture source models. In particular, we offer an asymptotic characterization of the behavior of the (upper bound to the) misclassification probability associated with the optimal Maximum-A-Posteriori (MAP) classifier that depends on quantities that are dual to the concepts of diversity gain and coding gain in multi-antenna communications. The diversity, which is shown to determine the rate at which the probability of misclassification decays in the low noise regime, is shown to depend on the geometry of the source, the geometry of the measurement system and their interplay. The measurement gain, which represents the counterpart of the coding gain, is also shown to depend on geometrical quantities. It is argued that the diversity order and the measurement gain also offer an optimization criterion to perform dictionary learning for compressive classification applications.},
archivePrefix = {arXiv},
arxivId = {1302.4660},
author = {Reboredo, Hugo and Renna, Francesco and Calderbank, Robert and Rodrigues, Miguel D.},
booktitle = {IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
eprint = {1302.4660},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Reboredo et al/2013 - Compressive Classification.pdf:pdf},
pages = {1029--1032},
title = {{Compressive Classification}},
url = {http://arxiv.org/abs/1302.4660},
year = {2013}
}
@article{Reynolds2000,
author = {Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn, Robert B.},
doi = {10.1006/dspr.1999.0361},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Reynolds, Quatieri, Dunn/2000 - Speaker Verification Using Adapted Gaussian Mixture Models.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing},
keywords = {Gaussian mixture models,NIST evaluation,gaussian mixture models,handset normalization,likelihood,likelihood ratio detector,nist,ratio detector,speaker recognition,universal background model},
number = {1-3},
pages = {19--41},
title = {{Speaker Verification Using Adapted Gaussian Mixture Models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1051200499903615},
volume = {10},
year = {2000}
}
@article{Reynolds1995,
author = {Reynolds, Douglas A. and Rose, Richard C.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Reynolds, Rose/1995 - Robust text-independent speaker identification using Gaussian mixture speaker models.pdf:pdf},
journal = {Speech and Audio Processing, IEEE Transactions},
number = {1},
title = {{Robust text-independent speaker identification using Gaussian mixture speaker models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=365379},
volume = {3},
year = {1995}
}
@book{Robinson2011,
abstract = {This accessible research monograph investigates how 'finite-dimensional' sets can be embedded into finite-dimensional Euclidean spaces. The first part brings together a number of abstract embedding results, and provides a unified treatment of four definitions of dimension that arise in disparate fields: Lebesgue covering dimension (from classical 'dimension theory'), Hausdorff dimension (from geometric measure theory), upper box-counting dimension (from dynamical systems), and Assouad dimension (from the theory of metric spaces). These abstract embedding results are applied in the second part of the book to the finite-dimensional global attractors that arise in certain infinite-dimensional dynamical systems, deducing practical consequences from the existence of such attractors: a version of the Takens time-delay embedding theorem valid in spatially extended systems, and a result on parametrisation by point values. This book will appeal to all researchers with an interest in dimension theory, particularly those working in dynamical systems.},
author = {Robinson, J. C.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Robinson/2011 - Dimensions, embeddings, and attractors.pdf:pdf},
isbn = {9780521898058},
keywords = {QA Mathematics},
title = {{Dimensions, embeddings, and attractors}},
url = {http://webcat.warwick.ac.uk/record=b2341614{~}S1},
year = {2011}
}
@inproceedings{Romero2013,
abstract = {Most research efforts in the field of compressed sensing have been pointed towards analyzing sampling and reconstruction techniques for sparse signals, where sampling rates below the Nyquist rate can be reached. When only second-order statistics or, equivalently, covariance information is of interest, perfect signal reconstruction is not required and rate reductions can be achieved even for non-sparse signals. This is what we will refer to as compressive covariance sampling. In this paper, we will study minimum-rate compressive covariance sampling designs within the class of non-uniform samplers. Necessary and sufficient conditions for perfect covariance reconstruction will be provided and connections to the well-known sparse ruler problem will be highlighted. {\textcopyright} 2013 IEEE.},
author = {Romero, Daniel and Leus, Geert},
booktitle = {Information Theory and Applications Workshop (ITA)},
doi = {10.1109/ITA.2013.6502949},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Romero, Leus/2013 - Compressive covariance sampling.pdf:pdf},
isbn = {978-1-4673-4647-4},
pages = {204--211},
title = {{Compressive covariance sampling}},
year = {2013}
}
@book{Rudin1987,
author = {Rudin, W},
edition = {cGraw-Hill},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Rudin/1987 - Real and complex analysis.pdf:pdf},
title = {{Real and complex analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Real-and-Complex-Analysis{\#}0},
year = {1987}
}
@book{Rudin1962,
author = {Rudin, W},
booktitle = {Interscience tracts in pure and applied mathematics},
doi = {10.1016/0041-5553(63)90263-5},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Rudin/1962 - Fourier Analysis on Groups.pdf:pdf},
isbn = {978-0471523642},
issn = {00415553},
publisher = {Interscience Publishers},
title = {{Fourier Analysis on Groups}},
year = {1962}
}
@article{Salas-Gonzalez2009,
author = {Salas-Gonzalez, Diego and Kuruoglu, Ercan E. and Ruiz, Diego P.},
doi = {10.1016/j.dsp.2007.11.004},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Salas-Gonzalez, Kuruoglu, Ruiz/2009 - Finite mixture of $\alpha$-stable distributions(2).pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing},
keywords = {bayesian inference,mixture models,monte carlo,reversible jump markov chain,stable distributions},
number = {2},
pages = {250--264},
publisher = {Elsevier Inc.},
title = {{Finite mixture of $\alpha$-stable distributions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1051200407001819},
volume = {19},
year = {2009}
}
@article{Scholkopf2015,
abstract = {We describe a method to perform functional operations on probability distributions of random variables. The method uses reproducing kernel Hilbert space representations of probability distributions, and it is applicable to all operations which can be applied to points drawn from the respective distributions. We refer to our approach as {\{}$\backslash$em kernel probabilistic programming{\}}. We illustrate it on synthetic data, and show how it can be used for nonparametric structural equation models, with an application to causal inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.06794v1},
author = {Sch{\"{o}}lkopf, Bernhard},
eprint = {arXiv:1501.06794v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sch{\"{o}}lkopf/2015 - Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations.pdf:pdf},
journal = {Statistics and Computing},
number = {4},
pages = {755--766},
title = {{Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations}},
volume = {25},
year = {2015}
}
@article{Slavakis2014,
author = {Slavakis, Konstantinos and Giannakis, Georgios B. and Mateos, Gonzalo},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Slavakis, Giannakis, Mateos/2014 - Modeling and Optimization for Big Data Analytics.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {5},
pages = {18--31},
title = {{Modeling and Optimization for Big Data Analytics}},
volume = {31},
year = {2014}
}
@inproceedings{Smola2007,
abstract = {We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.},
author = {Smola, Alexander J. and Gretton, Arthur and Song, Le and Sch{\"{o}}lkopf, Bernhard},
booktitle = {International Conference on Algorithmic Learning Theory},
doi = {10.1007/978-3-540-75225-7_5},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Smola et al/2007 - A Hilbert Space Embedding for Distributions.pdf:pdf},
isbn = {978-3-540-75224-0},
issn = {0302-9743},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {13--31},
title = {{A Hilbert Space Embedding for Distributions}},
url = {http://eprints.pascal-network.org/archive/00003987/},
year = {2007}
}
@techreport{Sridharan2002,
author = {Sridharan, Karthik},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sridharan/2002 - A Gentle Introduction to Concentration Inequalities.pdf:pdf},
title = {{A Gentle Introduction to Concentration Inequalities}},
url = {http://www.intertwingly.net/stories/2002/03/16/aGentleIntroductionToSoap.html},
year = {2002}
}
@inproceedings{Sriperumbudur2011,
abstract = {In this paper, we consider the problem of estimating a density using a finite combination of densities from a given class, C. Unlike previous works, where Kullback-Leibler (KL) divergence is used as a notion of distance, in this paper, we consider a distance measure based on the embedding of densities into a reproducing kernel Hilbert space (RKHS). We analyze the estimation and approximation errors for an M-estimator and show the estimation error rate to be better than that obtained with KL divergence while achieving the same approximation error rate. Another advantage of the Hilbert space embedding approach is that these results are achieved without making any assumptions on C, in contrast to the KL divergence approach, where the densities in C are assumed to be bounded (and away from zero) with C having a finite Dudley entropy integral.},
annote = {greedy algotihm that minimizes the kernel distance each time it adds a component

intuitively, does NOT work in practice, but theoretical guarantees on the rate of convergence (for asymptotically large number of components)

proofs are full of mistakes, like, everywhere. But results are correct (and can even be improved when corrections are made).},
author = {Sriperumbudur, Bharath K.},
booktitle = {IEEE International Symposium on Information Theory},
doi = {10.1109/ISIT.2011.6033685},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sriperumbudur/2011 - Mixture density estimation via hilbert space embedding of measures.pdf:pdf},
isbn = {9781457705953},
issn = {21578104},
pages = {1027--1030},
title = {{Mixture density estimation via hilbert space embedding of measures}},
year = {2011}
}
@inproceedings{Sriperumbudur2009,
abstract = {Page 1. Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions Bharath K. Sriperumbudur Department of ECE UC San Diego, La Jolla, USA bharathsv@ucsd.edu Kenji Fukumizu The Institute of Statistical},
author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Lanckriet, Gert R. G. and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Advances in Neural Information and Processing Systems (NIPS)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sriperumbudur et al/2009 - Kernel choice and classifiability for RKHS embeddings of probability distributions.pdf:pdf},
isbn = {978-1-615-67911-9},
pages = {1--13},
title = {{Kernel choice and classifiability for RKHS embeddings of probability distributions}},
url = {http://discovery.ucl.ac.uk/1334301/},
year = {2009}
}
@article{Sriperumbudur2010,
author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Sch{\"{o}}lkopf, Bernhard and Lanckriet, Gert R. G.},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sriperumbudur et al/2010 - Hilbert space embeddings and metrics on probability measures.pdf:pdf},
journal = {The Journal of Machine Learning Research},
pages = {1517--1561},
title = {{Hilbert space embeddings and metrics on probability measures}},
url = {http://dl.acm.org/citation.cfm?id=1859901},
volume = {11},
year = {2010}
}
@article{Sutherland2015,
annote = {approx kernel on distribution

level-2 kernels: rbf of distances between distributions (be that L2, Hell, KL, or even mean kernel (muandet))

Approx by embedding the distance into L2 (functions), then into Rn using an orthonormal basis of the cube, then use of RFF for the level 2 RBF.

In practice it is NOT possible to directly use empirical distrib for computing the kernel (it must be smooth), so non-parametric kernel estimate. argh ! (sampling high dimension, etc.)

Strong assumptions on p and q for theoretical results.

Use of sketch of sketches in the experiment (ie of the mean kernel distance of muandet, with RFF of mean RFFs (sketches)), scene classification.},
author = {Sutherland, Dougal J and Oliva, Junier B. and Barnabas, P and Schneider, Jeff},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Sutherland et al/2015 - Linear-time Learning on Distributions with Approximate Kernel Embeddings.pdf:pdf},
journal = {arXiv:1509.07553},
pages = {1--10},
title = {{Linear-time Learning on Distributions with Approximate Kernel Embeddings}},
year = {2015}
}
@inproceedings{Thaper2002,
address = {New York, New York, USA},
author = {Thaper, Nitin and Guha, Sudipto and Indyk, Piotr and Koudas, Nick},
booktitle = {International conference on Management of data (SIGMOD)},
doi = {10.1145/564740.564741},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Thaper et al/2002 - Dynamic multidimensional histograms.pdf:pdf},
isbn = {1581134975},
pages = {428--439},
publisher = {ACM Press},
title = {{Dynamic multidimensional histograms}},
url = {http://portal.acm.org/citation.cfm?doid=564691.564741},
year = {2002}
}
@article{Tran1998,
abstract = {This paper uses the empirical characteristic function (ECF) procedure to estimate the parameters of mixtures of normal distributions. Since the characteristic function is uniformly bounded, the procedure gives estimates that are numerically stable. It is shown that, using Monte Carlo simulation, the finite sample properties of th ECF estimator are very good, even in the case where the popular maximum likelihood estimator fails to exist. An empirical application is illustrated using the monthl excess return of the Nyse value-weighted index.},
author = {Tran, Kien C.},
doi = {10.1080/07474939808800410},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Tran/1998 - Estimating mixtures of normal distributions via empirical characteristic function.pdf:pdf},
issn = {0747-4938},
journal = {Econometric Reviews},
number = {2},
pages = {167--183},
title = {{Estimating mixtures of normal distributions via empirical characteristic function}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07474939808800410$\backslash$nhttp://www.tandfonline.com.ezproxy.uniandes.edu.co:8080/doi/abs/10.1080/07474939808800410{\#}.UhU7UH-wVfs$\backslash$nhttp://www.tandfonline.com.ezproxy.uniandes.edu.co:8080/doi/pdf/10.1080/07474939808800410},
volume = {17},
year = {1998}
}
@techreport{Vedaldi2010,
abstract = {E-participation is a relatively new approach, so it is necessary to evaluate it carefully so that we can improve e-participation practice. This paper describes a framework that has been developed for evaluating a number of e-participation pilots in the legislation development processes of parliaments. The framework is based on the objectives and basic characteristics of 'traditional' public participation, e-participation and the legislation development processes, as well as the existing frameworks for the evaluation of Information Systems (ISs), e-participation and traditional public participation. It includes three perspectives: process, system and outcomes evaluation; each of them is analysed into a number of evaluation criteria.},
author = {Vedaldi, Andrea and Fulkerson, Brian},
doi = {10.1145/1873951.1874249},
isbn = {9781605589336},
keywords = {computer vision,image classification,object recognition,vi},
title = {{VLFeat - An open and portable library of computer vision algorithms}},
url = {http://vision.ucla.edu/{~}brian/papers/vedaldi10vlfeat.pdf},
year = {2010}
}
@article{Vedaldi2012,
abstract = {Maji and Berg have recently introduced an explicit feature map approximating the intersection kernel. This enables efficient learning methods for linear kernels to be applied to the non-linear intersection kernel, expanding the applicability of this model to much larger problems. In this paper we generalize this idea, and analyse a large family of additive kernels, called homogeneous, in a unified framework. The family includes the intersection, Hellinger's, and {\&}{\#}x03C7;{\textless}sup{\textgreater}2{\textless}/sup{\textgreater} kernels commonly employed in computer vision. Using the framework we are able to: (i) provide explicit feature maps for all homogeneous additive kernels along with closed form expression for all common kernels; (ii) derive corresponding approximate finite-dimensional feature maps based on the Fourier sampling theorem; and (iii) quantify the extent of the approximation. We demonstrate that the approximations have indistinguishable performance from the full kernel on a number of standard datasets, yet greatly reduce the train/test times of SVM implementations. We show that the {\&}{\#}x03C7;{\textless}sup{\textgreater}2{\textless}/sup{\textgreater} kernel, which has been found to yield the best performance in most applications, also has the most compact feature representation. Given these train/test advantages we are able to obtain a significant performance improvement over current state of the art results based on the intersection kernel.},
author = {Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.1109/TPAMI.2011.153},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Vedaldi, Zisserman/2012 - Efficient additive kernels via explicit feature maps.pdf:pdf},
isbn = {9781424469840},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Kernel methods,feature map,large scale learning,object detection,object recognition},
number = {3},
pages = {480--492},
pmid = {21808094},
title = {{Efficient additive kernels via explicit feature maps}},
volume = {34},
year = {2012}
}
@article{Verbeek2003,
abstract = {This article concerns the greedy learning of gaussian mixtures. In the greedy approach, mixture components are inserted into the mixture one after the other. We propose a heuristic for searching for the optimal component to insert. In a randomized manner, a set of candidate new components is generated. For each of these candidates, we find the locally optimal new component and insert it into the existing mixture. The resulting algorithm resolves the sensitivity to initialization of state-of-the-art methods, like expectation maximization, and has running time linear in the number of data points and quadratic in the (final) number of mixture components. Due to its greedy nature, the algorithm can be particularly useful when the optimal number of mixture components is unknown. Experimental results comparing the proposed algorithm to other methods on density estimation and texture segmentation are provided.},
author = {Verbeek, Jakob and Vlassis, Nikos and Kr{\"{o}}se, Ben},
doi = {10.1162/089976603762553004},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Verbeek, Vlassis, Kr{\"{o}}se/2003 - Efficient greedy learning of gaussian mixture models.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Learning: physiology,Normal Distribution},
number = {2},
pages = {469--485},
pmid = {12590816},
title = {{Efficient greedy learning of gaussian mixture models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12590816},
volume = {15},
year = {2003}
}
@inproceedings{Volkov2008,
author = {Volkov, Vasily and Demmel, James},
booktitle = {High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/SC.2008.5214359},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Volkov, Demmel/2008 - Benchmarking g GPUs to Tune Dense Linear Algebra.pdf:pdf},
isbn = {978-1-4244-2834-2},
pages = {1--11},
title = {{Benchmarking g GPUs to Tune Dense Linear Algebra}},
year = {2008}
}
@inproceedings{Wang2014,
annote = {Non-linear Compressed sensing for both recovery and classification in the measurement domain
































































non linear expansion, then linear combination CS-like
































































Design of measurement by maximization of mutual information, focus on polynomial expansion
































































Gradient of the mutual information with respect to the linear part of the measurement (after non-linear expansion), allow for optimization of the design (via MMSE matrix, and Monte Carlo)
































































the linear stuff is designed ! (and not random)






























































































































































































explicit bounds between mutual information and MMSE, etc},
author = {Wang, Liming and Razi, Abolfazl and Rodrigues, Miguel D. and Calderbank, Robert and Carin, Lawrence},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Wang et al/2014 - Nonlinear Information-Theoretic Compressive Measurement Design.pdf:pdf},
pages = {1161--1169},
title = {{Nonlinear Information-Theoretic Compressive Measurement Design}},
url = {http://people.duke.edu/{~}lw174/pub/ICML2014.pdf},
year = {2014}
}
@article{Xu2010,
author = {Xu, Dinghai and Knight, John},
doi = {10.1080/07474938.2011.520565},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Xu, Knight/2010 - Continuous Empirical Characteristic Function Estimation of Mixtures of Normal Parameters.pdf:pdf},
issn = {0747-4938},
journal = {Econometric Reviews},
keywords = {c15,c16,empirical characteristic function,jel classification c13,mixtures of normal},
number = {1},
pages = {25--50},
title = {{Continuous Empirical Characteristic Function Estimation of Mixtures of Normal Parameters}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2011.520565},
volume = {30},
year = {2010}
}
@article{Yu2011,
annote = {Statistical compressed sensing, achieving good reconstruction $\backslash$emph{\{}on average{\}} (also in $\backslash$citet{\{}Cohen2009{\}}).
















Performance bound for single gaussian requiring much less measurements than usual CS {\$}O(k){\$}, with same random matrices.
















optimal decoder for gaussian: linear filtering (no basis pursuit or whatever)},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.5785v1},
author = {Yu, Guoshen and Sapiro, Guillermo},
eprint = {arXiv:1101.5785v1},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Yu, Sapiro/2011 - Statistical compressed sensing of Gaussian mixture models.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {12},
pages = {1--30},
title = {{Statistical compressed sensing of Gaussian mixture models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6021388},
volume = {59},
year = {2011}
}
@incollection{Zhang2015,
author = {Zhang, Peng and Gao, Yuxiang},
booktitle = {ISC High Performance},
doi = {10.1007/978-3-540-77304-7},
editor = {Kunkel, M. Julian and Ludwig, Thomas},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Zhang, Gao/2015 - Matrix Multiplication on High-Density Multi-GPU Architectures Theoretical and Experimental Investigations Peng.pdf:pdf},
isbn = {9783540773047},
keywords = {architectures,architectures {\'{a}} high-density multi-gpu,evaluation,matrix multiplication {\'{a}} performance},
pages = {17--30},
publisher = {Springer International Publishing},
title = {{Matrix Multiplication on High-Density Multi-GPU Architectures: Theoretical and Experimental Investigations Peng}},
volume = {1},
year = {2015}
}
@article{Zhao2014,
abstract = {The maximum mean discrepancy (MMD) is a recently proposed test statistic for two-sample test. Its quadratic time complexity, however, greatly hampers its availability to large-scale applications. To accelerate the MMD calculation, in this study we propose an efficient method called FastMMD. The core idea of FastMMD is to equivalently transform the MMD with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on Bochner's theorem and Fourier transform $\backslash$cite{\{}Rahimi07{\}}. Taking advantage of sampling of Fourier transform, FastMMD decreases the time complexity for MMD calculation from {\$}O(N{\^{}}2 d){\$} to {\$}O(N d){\$}, where {\$}N{\$} and {\$}d{\$} are the size and dimension of the sample set, respectively. For kernels that are spherically invariant, the computation can be further accelerated to {\$}O(N \backslashlog d){\$} by using the Fastfood technique $\backslash$cite{\{}LeQ13{\}}. The uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates. We have further provided a geometric explanation for our method, namely ensemble of circular discrepancy, which facilitates us to understand the insight of MMD, and is hopeful to help arouse more extensive metrics for assessing two-sample test. Experimental results substantiate that FastMMD is with similar accuracy as exact MMD, while with faster computation speed and lower variance than the existing MMD approximation methods.},
archivePrefix = {arXiv},
arxivId = {1405.2664},
author = {Zhao, Ji and Meng, Deyu},
doi = {10.1162/NECO_a_00732},
eprint = {1405.2664},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Zhao, Meng/2015 - FastMMD Ensemble of Circular Discrepancy for Efficient Two-Sample Test.pdf:pdf},
journal = {Journal Neural Computation},
keywords = {fastfood,maximum mean discrepancy,mmd,ran-,two-sample test},
number = {6},
pages = {1345--1372},
title = {{FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test}},
url = {http://arxiv.org/abs/1405.2664},
volume = {27},
year = {2015}
}
@inproceedings{Zhou2008,
author = {Zhou, Xi and Zhuang, Xiaodan and Yan, Shuicheng and Chnag, Shih-Fu and Hasegawa-Johnson, Mark and Huang, Thomas S.},
booktitle = {ACM International Conference on Multimedia},
file = {:C$\backslash$:/Users/nkeriven/Documents/mendeley/Zhou et al/2008 - SIFT-Bag Kernel for Video Event Analysis.pdf:pdf},
isbn = {9781605583037},
keywords = {kernel design,malization,sift-bag,video event recognition,within-class covariation nor-},
pages = {229--238},
title = {{SIFT-Bag Kernel for Video Event Analysis}},
year = {2008}
}

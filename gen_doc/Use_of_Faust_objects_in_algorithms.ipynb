{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the FAµST API in Algorithms\n",
    "\n",
    "After the little tour we've done in the previous notebooks, about the [creation of Faust objects](#creation_links), their [manipulation](#manip_links) and their creation using [factorization](#facto_links) algorithms, we shall see in this fourth notebook how the FAµST API can be deployed seamlessly in algorithms.\n",
    "Our example, already alluded in the [second notebook](#manip_links), will be the Orthogonal Matching Pursuit algorithm (OMP).\n",
    "\n",
    "This algorithm intervenes in the dictionary learning problem. Of course, I will not treat the theory behind but I assume the reader is already familiar with.\n",
    "There is not so much to say so let's go straight to the code example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The OMP Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaust import *\n",
    "from numpy import zeros, copy, argmax\n",
    "\n",
    "def tomp(y, D, niter):\n",
    "    nrows, ncols = D.shape\n",
    "    x = zeros((ncols,1))\n",
    "    supp = []\n",
    "    res = copy(y)\n",
    "    i = 0\n",
    "    K = min(nrows, ncols)\n",
    "    while (len(supp) < K and i < niter):\n",
    "        j = argmax(abs(D.T*res))\n",
    "        supp += [j] \n",
    "        x[supp,:] = pinv(D[:,supp])*y\n",
    "        res = y-D[:,supp]*x[supp,:]\n",
    "        i += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important point to notice in this code is that except the import part in the header, all the code seems to be a natural numpy implementation of OMP.  \n",
    "This is in fact the core philosophy of the FAµST API, as explained in previous notebooks and also in the API documentation, we made sure that a Faust can be seen as a numpy array (or rather as a ``numpy.matrix``) hence this code is in fact totally compatible with the two APIs: the D function argument, which is the dictionary, can be indifferently a ``pyfaust.Faust`` object or a ``numpy.matrix`` object.  \n",
    "A secondary point is that this implementation is more like a toy concept (as indicated by the \"t\" in the function name). A more advanced and optimized version is introduced in the [last part of this notebook](#4.-A-Greedy-OMP-Cholesky-Implementation) and in particular allows to define the algorithm stopping criterion according to the error tolerance the user wants.\n",
    "\n",
    "Next we will test this implementation in both cases. But first, let us define a test case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Test Case Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we shall set up a dictionary which garantees uniqueness of sufficiently sparse representations.\n",
    "The dictionary is the concatenation of an identity matrix and a Hadamard matrix, and because we work with Faust objects, this concatenation will be a Faust object.\n",
    "\n",
    "Below is the block matrix of our dictionary:  \n",
    "$\n",
    "D =\n",
    "\\left[\n",
    "\\begin{array}{c|c}\n",
    "I_n & H_n \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "$I_n$ is the identity or Dirac matrix and $H_n$ the orthonormal Hadamard matrix, with n being a power of two.\n",
    "\n",
    "The condition on which the uniqueness of the sparse representation $x$ of a vector $y$  is insured is defined by the following inequality:  \n",
    "$ \\| x \\|_0 < (1 + 1/\\mu)/2 $ where $\\mu$ denotes the coherence of the dictionary and in case of our specially crafted dictionary $\\mu = {1 \\over \\sqrt n}$.\n",
    "\n",
    "So let's construct the Faust of D, compute y for a unique sparse x and test our OMP implementation to find out if we effectively retrieve the unique x as we should according to this theorem.\n",
    "\n",
    "Note that, for a better view and understanding you might consult this article [[1]](#[1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyfaust import FaustFactory as FF\n",
    "from numpy import log2\n",
    "n = 128\n",
    "FD = hstack((FF.eye(n),FF.wht(n)))\n",
    "D = FD.todense()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dictionary both defined as a Faust (FD) and as a matrix (D), let's construct our reference sparse vector x, we'll call it $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, count_nonzero\n",
    "from numpy.random import randn, permutation as randperm\n",
    "from random import randint\n",
    "from math import floor, sqrt\n",
    "x0 = zeros((2*n, 1)) # NB: FD.shape[1] == 2*n\n",
    "nnz = floor(.5*(1+sqrt(n)))\n",
    "nonzero_inds = randperm(2*n)[:nnz]\n",
    "# we got nnz indices, now build the vector x0\n",
    "x0[nonzero_inds,0] = randn()\n",
    "print(\"l0 norm of x0: \", count_nonzero(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains to compute $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = FD*x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test case is complete, we are fully prepared to run the OMP algorithm using a well-defined dictionary as a Faust or as numpy array, this should retrieve our $x_0$ from the vector y. Let's try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tomp(y, FD, nnz)\n",
    "from numpy import allclose\n",
    "assert(allclose(x,x0))\n",
    "print(\"We succeeded to retrieve x0, OMP works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested OMP on a Faust, go ahead and verify what I was aiming at in the first part of the notebook: is this OMP implementation really working identically on a Faust and a ``numpy.matrix``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tomp(y, D, nnz)\n",
    "assert(allclose(x,x0))\n",
    "print(\"We succeeded to retrieve x0, OMP works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the algorithm is indeed available to both numpy and Faust worlds, and we can imagine surely that other algorithms are reachable through the FAµST API. That's anyway in that purpose that the FAµST library will be extended in the future (starting from the latest version which is 2.5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. An OMP-Cholesky Implementation\n",
    "\n",
    "Speaking of the OMP algorithm and the possibility to implement other optimization algorithms with FAµST, it would be a pity not to mention that the library is delivered with another implementation of OMP.  \n",
    "This implementation is actually an optimized version which takes advantage of the Cholesky factorization to simplify the least-square problem to solve at each iteration.\n",
    "This algorithm is implemented into the ``tools`` module of pyfaust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaust.tools import omp\n",
    "help(omp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is integrated into pyfaust as a tool for the Brain Source Localization demo which is documented [here](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/classpyfaust_1_1demo_1_1bsl.html).  \n",
    "To show off a little, let's run this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "from pyfaust.demo import bsl\n",
    "bsl.run() # it will take some time (sorry), many Faust-s are compared to the original MEG matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%matplotlib inline\n",
    "from pyfaust.demo import bsl\n",
    "bsl.fig_time_cmp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see in this figure is that it takes a few dozens of milliseconds (the median time) to compute the BSL experiment on the the matrix M. This is well above the time it takes with Faust approximates $\\hat M_6$ to $\\hat M_{23}$ in which the numbers 6 and 23 denote the Faust [RCG](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/classpyfaust_1_1Faust.html#a6a51a05c20041504a0b8f2a73dd8d05a). The greater the RCG the better the computation time is, as we already saw in the [notebook](#manip_links) about Faust manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a complementary test, let's verify that the two runs of ``omp()`` on FD and D we constructed before for the toy omp give the same results even if the vector to retrieve is not sparse. Here for instance, $ \\| x_1 \\|_0 = 98 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz = 98\n",
    "x1 = zeros((2*n, 1))\n",
    "nnz_inds = randperm(2*n)[:nnz]\n",
    "x1[nnz_inds, 0] = randn()\n",
    "y = FD*x1\n",
    "x2 = omp(y, D, maxiter=nnz-1)\n",
    "x3 = omp(y, FD, maxiter=nnz-1)\n",
    "# verify if the solutions differ\n",
    "print(\"Are x2 and x3 solutions almost equal?\", norm(x2-x3)/norm(x3) < 10**-12)\n",
    "print(\"Is x1 retrieved into x2?\", allclose(x1,x2))\n",
    "print(\"Is x1 retrieved into x3?\", allclose(x1,x3))\n",
    "print(\"x3 mse: \",norm(y-D*x2)**2/y.shape[0])\n",
    "print(\"x2 mse: \",norm(y-FD*x3)**2/y.shape[0])\n",
    "print(\"x2 and x3 nnz:\", count_nonzero(x2), count_nonzero(x3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we didn't retrieve our starting x1 (the reason is the condition already discussed in 1.). However we did even better by finding sparser representations. Anyway let's mention that here again like it was with the toy OMP it works the same with the Faust API or with numpy.\n",
    "\n",
    "Finally, let's check the compute time for applying our dictionary to a vector both for the numpy and Faust versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit D*x2\n",
    "%timeit FD*x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The fourth notebook is ending here***, I hope you'll be interested in trying yourself to write another algorithm with the FAµST API and maybe discovering any current limitation. Don't hesitate to contact us in that case, we'll appreciate any feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "<a name=\"creation_links\">Faust creation (1st) notebook: </a> [html](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_creation.html), [ipynb](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_creation.ipynb)  \n",
    "<a name=\"manip_links\">Faust manipulation (2nd) notebook:</a> [html](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_manipulation.html), [ipynb](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_manipulation.ipynb)  \n",
    "<a name=\"facto_links\">Factorization (3rd) notebook: </a>[html](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_factorization.html), [ipynb](https://faustgrp.gitlabpages.inria.fr/faust/last-doc/html/Faust_factorization.ipynb)  \n",
    "<a name=\"[1]\">[1]</a> [Tropp, J. A. (2004). Greed is Good: Algorithmic Results for Sparse Approximation. IEEE Transactions on Information Theory, 50(10), 2231–2242](http://doi.org/10.1109/TIT.2004.834793)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
